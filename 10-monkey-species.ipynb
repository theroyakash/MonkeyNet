{"cells":[{"metadata":{"_uuid":"0339b188-a3c4-436d-b445-33664960113e","_cell_guid":"e0dc0518-7560-4b19-876e-b196414af322","trusted":true},"cell_type":"markdown","source":"# Necessary imports and Data Loading"},{"metadata":{"_uuid":"ac1c88d6-3b40-4587-9f92-5c57399bdd97","_cell_guid":"ae31eda8-1e9b-44fb-a1d1-fba1ce09852b","trusted":true},"cell_type":"code","source":"import numpy as np\nfrom matplotlib import pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","execution_count":74,"outputs":[]},{"metadata":{"_uuid":"84a2f4fa-da1b-4522-b74d-bffb9b56d5bb","_cell_guid":"58de35df-7802-4f8b-91d6-f137ccd7a634","trusted":true},"cell_type":"markdown","source":"## Loading Data"},{"metadata":{"_uuid":"40920648-1c52-42da-9b6f-581c0d224ffc","_cell_guid":"4ac7bc32-76ad-453b-9447-f322edefe433","trusted":true},"cell_type":"code","source":"data_set_directory = '../input/10-monkey-species'\ntraining_path = data_set_directory + '/training/training'\nvalidation_path = data_set_directory + '/validation/validation'","execution_count":75,"outputs":[]},{"metadata":{"_uuid":"bf151892-9071-4f19-8b0c-e328cabc3b8e","_cell_guid":"e9da5e41-a0bd-42c9-a6aa-58b07ecfb1f1","trusted":true},"cell_type":"code","source":"image_size = (224, 224)\nbatch_size = 64\n\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    training_path,\n    label_mode = 'int',\n    seed = 1337,\n    image_size=image_size,\n    batch_size=batch_size,\n)\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    validation_path,\n    label_mode = 'int',\n    seed =1337,\n    image_size=image_size,\n    batch_size=batch_size,\n)","execution_count":76,"outputs":[{"output_type":"stream","text":"Found 1097 files belonging to 10 classes.\nFound 272 files belonging to 10 classes.\n","name":"stdout"}]},{"metadata":{"_uuid":"dd98fe5c-0945-4290-b6a8-a7185376658a","_cell_guid":"15ea3a54-68e5-4881-a3a4-724c7d805702","trusted":true},"cell_type":"markdown","source":"# Training data augmentation"},{"metadata":{"_uuid":"f6b88905-0691-435b-b905-9c3eeb6c1967","_cell_guid":"13b92601-7006-45ba-89c7-fec687998a25","trusted":true},"cell_type":"code","source":"# Data Augmentation Layer for training\n\ndata_augmentation_train = keras.Sequential(\n    [\n        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n        layers.experimental.preprocessing.RandomRotation(0.1),\n        layers.experimental.preprocessing.Rescaling(scale =1./255),\n        layers.experimental.preprocessing.RandomHeight(0.1),\n        layers.experimental.preprocessing.RandomWidth(0.1)\n     \n    ]\n)","execution_count":77,"outputs":[]},{"metadata":{"_uuid":"69a83269-4f1b-402f-8941-5ac45d2214a3","_cell_guid":"64723faf-48b6-4a87-985a-fb3ecece4745","trusted":true},"cell_type":"markdown","source":"# Testing data augmentation"},{"metadata":{"_uuid":"16b94d94-1c27-4843-b066-a58338d2d647","_cell_guid":"0d4cc9f1-af42-480f-a0d6-c33d1ba16616","trusted":true},"cell_type":"code","source":"# Data Augmentation Layer for testing\n\ndata_augmentation_test = keras.Sequential(\n    [\n        layers.experimental.preprocessing.Rescaling(scale =1./255)\n     \n    ]\n)","execution_count":78,"outputs":[]},{"metadata":{"_uuid":"3f99c6d6-6d78-4d19-87cd-5b1072fd0cea","_cell_guid":"82e7fe03-3c66-47d2-9a40-4b511805966b","trusted":true},"cell_type":"markdown","source":"Mapping the augmentations into the dataset"},{"metadata":{"_uuid":"72433df8-dcf5-462e-a1f7-d187ede49d02","_cell_guid":"c0abec7d-e8b8-482a-8a6d-c150d3fe9b2a","trusted":true},"cell_type":"code","source":"# Map the Data now\n\naugmented_train_ds = train_ds.map(\n  lambda x, y: (data_augmentation_train(x, training=True), y))\n\naugmented_val_ds = val_ds.map(\n  lambda x, y: (data_augmentation_test(x, training=True), y))","execution_count":79,"outputs":[]},{"metadata":{"_uuid":"0f4f14ba-deb1-44fd-b0a7-0243b7237430","_cell_guid":"e9440796-dec6-4ed9-bfa0-eea6dfa3d4ea","trusted":true},"cell_type":"markdown","source":"# Training Using the Transfer learning\nSee below for the training using a custom deep neural architecture"},{"metadata":{"_uuid":"02ec09fa-5716-4b6d-b178-182672aa96bd","_cell_guid":"38b4ee25-2905-499d-9621-b467c7da6b78","trusted":true},"cell_type":"code","source":"from tensorflow.keras.applications.inception_v3 import InceptionV3\n\npre_trained_model = InceptionV3(input_shape=(224,224,3), include_top=False, weights='imagenet')\n\nfor layer in pre_trained_model.layers:\n    layer.trainable = False","execution_count":80,"outputs":[]},{"metadata":{"_uuid":"a2877285-793d-4e8c-9124-23f680bf8e97","_cell_guid":"6e708cd2-7241-4206-8e0b-aec82b579059","trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import Adam\nfrom keras import regularizers","execution_count":81,"outputs":[]},{"metadata":{"_uuid":"a9145325-5b17-4521-8945-00a0cb7de4c2","_cell_guid":"47f1cc09-cb4d-48eb-87b9-0216d61efeea","trusted":true},"cell_type":"code","source":"global_average_layers = tf.keras.layers.GlobalAveragePooling2D()\ndropouts = tf.keras.layers.Dropout(rate = 0.2)\n\ndense_128 = tf.keras.layers.Dense(128)\ndense_64 = tf.keras.layers.Dense(64)\ndense_32 = tf.keras.layers.Dense(32)\n\nprediction_layer = tf.keras.layers.Dense(10,activation='softmax')\n\nmodel_V3 = tf.keras.Sequential([\n    pre_trained_model,\n    global_average_layers,\n    dropouts,\n    dense_128,\n    dense_64,\n    dropouts,\n    dense_32,\n    prediction_layer\n])\n\nmodel_V3.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","execution_count":87,"outputs":[]},{"metadata":{"_uuid":"b79362e3-cd45-4af3-b7fb-49bfd4ef2b55","_cell_guid":"26da0e71-a1ad-482f-b1b1-de3d8b641229","trusted":true},"cell_type":"code","source":"model_V3.summary()","execution_count":88,"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninception_v3 (Functional)    (None, 5, 5, 2048)        21802784  \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 2048)              0         \n_________________________________________________________________\ndropout (Dropout)            multiple                  0         \n_________________________________________________________________\ndense (Dense)                (None, 128)               262272    \n_________________________________________________________________\ndense_1 (Dense)              (None, 64)                8256      \n_________________________________________________________________\ndense_2 (Dense)              (None, 32)                2080      \n_________________________________________________________________\ndense_3 (Dense)              (None, 10)                330       \n=================================================================\nTotal params: 22,075,722\nTrainable params: 272,938\nNon-trainable params: 21,802,784\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"_uuid":"a49f7a7d-8c06-45a2-9026-2052c63c576f","_cell_guid":"7c25b697-66ee-4492-9d39-62939d96cd59","trusted":true},"cell_type":"code","source":"# Callbacks\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n\nfilepath = \"custom_training_model.h5\"\n\ntensorboard_callback = tf.keras.callbacks.TensorBoard(\"logs\")\nlr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=2, mode='max')\nearly_stop = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=1, mode='min')\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n\ncallback_list = [tensorboard_callback, lr_reduce, checkpoint]","execution_count":89,"outputs":[]},{"metadata":{"_uuid":"853b003e-9ca1-43c2-8c97-c472d6dad90b","_cell_guid":"e1cceaa6-d1f0-4aa8-9d7a-2b29e5514469","trusted":true},"cell_type":"code","source":"tf.keras.backend.clear_session()","execution_count":90,"outputs":[]},{"metadata":{"_uuid":"8b7099de-f18b-4e6f-84e4-67e5690b1647","_cell_guid":"16d5524d-e9c2-4f4d-a3a7-3fec748da4fe","trusted":true},"cell_type":"code","source":"history = model_V3.fit(\n           augmented_train_ds, \n           epochs=6, validation_data=augmented_val_ds, callbacks=callback_list)","execution_count":91,"outputs":[{"output_type":"stream","text":"Epoch 1/6\n18/18 [==============================] - ETA: 0s - loss: 1.8802 - accuracy: 0.5861\nEpoch 00001: val_loss improved from inf to 0.22962, saving model to custom_training_model.h5\n18/18 [==============================] - 21s 1s/step - loss: 1.8802 - accuracy: 0.5861 - val_loss: 0.2296 - val_accuracy: 0.9412\nEpoch 2/6\n18/18 [==============================] - ETA: 0s - loss: 0.3825 - accuracy: 0.8888\nEpoch 00002: val_loss improved from 0.22962 to 0.17638, saving model to custom_training_model.h5\n18/18 [==============================] - 18s 1s/step - loss: 0.3825 - accuracy: 0.8888 - val_loss: 0.1764 - val_accuracy: 0.9596\nEpoch 3/6\n18/18 [==============================] - ETA: 0s - loss: 0.2412 - accuracy: 0.9253\nEpoch 00003: val_loss improved from 0.17638 to 0.16558, saving model to custom_training_model.h5\n18/18 [==============================] - 19s 1s/step - loss: 0.2412 - accuracy: 0.9253 - val_loss: 0.1656 - val_accuracy: 0.9669\nEpoch 4/6\n18/18 [==============================] - ETA: 0s - loss: 0.2057 - accuracy: 0.9453\nEpoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n\nEpoch 00004: val_loss improved from 0.16558 to 0.15853, saving model to custom_training_model.h5\n18/18 [==============================] - 18s 994ms/step - loss: 0.2057 - accuracy: 0.9453 - val_loss: 0.1585 - val_accuracy: 0.9632\nEpoch 5/6\n18/18 [==============================] - ETA: 0s - loss: 0.1498 - accuracy: 0.9581\nEpoch 00005: val_loss improved from 0.15853 to 0.11459, saving model to custom_training_model.h5\n18/18 [==============================] - 18s 992ms/step - loss: 0.1498 - accuracy: 0.9581 - val_loss: 0.1146 - val_accuracy: 0.9706\nEpoch 6/6\n18/18 [==============================] - ETA: 0s - loss: 0.1607 - accuracy: 0.9508\nEpoch 00006: val_loss improved from 0.11459 to 0.10392, saving model to custom_training_model.h5\n18/18 [==============================] - 18s 986ms/step - loss: 0.1607 - accuracy: 0.9508 - val_loss: 0.1039 - val_accuracy: 0.9743\n","name":"stdout"}]},{"metadata":{"_uuid":"4c04d259-f7e5-4f3b-b2a2-ca5450f3b2d9","_cell_guid":"4ad7ffcf-3532-4e2c-8dfb-96e16adb0150","trusted":true},"cell_type":"code","source":"try:\n    del model_V3\nexcept Exception as e:\n    print(e)","execution_count":92,"outputs":[]},{"metadata":{"_uuid":"999eb9ea-8d9f-4c21-96bc-07bc25120b30","_cell_guid":"87dcc4b5-7936-4c86-aff7-dad43038f882","trusted":true},"cell_type":"code","source":"model = keras.models.load_model('custom_training_model.h5')","execution_count":93,"outputs":[]},{"metadata":{"_uuid":"d72b8715-d102-4981-9e23-50fb0aa0adb2","_cell_guid":"57872b7a-9847-4232-a4e8-f5c958a8e3d5","trusted":true},"cell_type":"code","source":"import numpy as np\nfrom keras.preprocessing import image\nfrom PIL import Image\nimport requests\nimport io\nimport os\n\ndef prepare_from_url(link):\n    bytesdata = requests.get(link).content\n    Image.open(io.BytesIO(bytesdata)).save('hehe.jpg')\n    \n    img_width, img_height = 224, 224\n    img = image.load_img('hehe.jpg', target_size = (img_width, img_height))\n    img = image.img_to_array(img)\n    img = np.expand_dims(img, axis = 0)\n    \n    os.remove(\"hehe.jpg\")\n\n    return img\n    \nimg = prepare_from_url('https://upload.wikimedia.org/wikipedia/commons/d/de/Silvery_Marmoset_%2810511829704%29.jpg')\npred = model.predict(img)","execution_count":94,"outputs":[]},{"metadata":{"_uuid":"baa37dfb-5cf2-4ca5-9c88-61f5522c2c19","_cell_guid":"407537b7-3f36-431e-9048-bdee279a835d","trusted":true},"cell_type":"code","source":"pred","execution_count":95,"outputs":[{"output_type":"execute_result","execution_count":95,"data":{"text/plain":"array([[0.0000000e+00, 7.4124638e-29, 0.0000000e+00, 1.0000000e+00,\n        0.0000000e+00, 0.0000000e+00, 2.8951564e-20, 0.0000000e+00,\n        5.7335915e-29, 0.0000000e+00]], dtype=float32)"},"metadata":{}}]},{"metadata":{"_uuid":"0addc855-1d69-427e-829d-0a980c72b797","_cell_guid":"33e71cdb-074a-44e8-91ef-a710585e8ba3","trusted":true},"cell_type":"code","source":"labels_latin = ['alouatta_palliata', \n          'erythrocebus_patas', \n          'cacajao_calvus', \n          'macaca_fuscata', \n          'cebuella_pygmea', \n          'cebus_capucinus', \n          'mico_argentatus', \n          'saimiri_sciureus', \n          'aotus_nigriceps', \n          'trachypithecus_johnii']\n\nlabels_common = ['mantled_howler', \n                 'patas_monkey', \n                 'bald_uakari', \n                 'japanese_macaque',\n                 'pygmy_marmoset',\n                 'white_headed_capuchin',\n                 'silvery_marmoset',\n                 'common_squirrel_monkey',\n                 'black_headed_night_monkey',\n                 'nilgiri_langur']\n\nprint(f'Model has predicted the image to be an {labels_common[np.argmax(pred)]} whose latin name is {labels_latin[np.argmax(pred)]}')","execution_count":96,"outputs":[{"output_type":"stream","text":"Model has predicted the image to be an japanese_macaque whose latin name is macaca_fuscata\n","name":"stdout"}]},{"metadata":{"_uuid":"13762a26-51dc-4e0e-9104-a2585852f72c","_cell_guid":"a6367168-c4e7-4f55-87cc-694df17eeb2a","trusted":true},"cell_type":"code","source":"!tensorboard dev upload --logdir log \\\n    --name \"MonkeyNet for monkey classification\" \\\n    --description \"Made by theroyakash\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78130cc3-220a-4bf1-98b5-5f39b9e2857b","_cell_guid":"091746da-ab11-426c-a285-75f1d3414381","trusted":true},"cell_type":"markdown","source":"# Using MobileNet v2"},{"metadata":{"_uuid":"ed7772a3-deaf-4204-aab5-505486c4e697","_cell_guid":"70e79197-3bfc-4d58-bdf4-7cb625285801","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}